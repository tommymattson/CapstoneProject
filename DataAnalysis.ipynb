{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Data Analysis Tools\n",
    "Thomas Mattson\n",
    "\n",
    "Westmont College - CS-195 Senior Seminar\n",
    "\n",
    "April 29, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Prompt Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Channel                            ID  \\\n",
      "0  newbies 3    563,806,429,635,543,000.00   \n",
      "1  newbies 3    775,578,578,561,531,000.00   \n",
      "2  newbies 3  1,089,046,680,424,410,000.00   \n",
      "3  newbies 3    456,383,955,697,008,000.00   \n",
      "4  newbies 3  1,016,095,568,788,990,000.00   \n",
      "\n",
      "                                              Prompt       Date  \\\n",
      "0  an indian tech nerd dressed like Tom Cruise in...  3/29/2023   \n",
      "1  art for podcast named Taste Buds that is hoste...  3/29/2023   \n",
      "2  8k photo realistic great white shark on beach ...  3/29/2023   \n",
      "3  wide angle full body shot  group of agressive ...  3/29/2023   \n",
      "4                   artwork by michel granger skull   3/29/2023   \n",
      "\n",
      "                                      Tokenized Text  \\\n",
      "0  [tech, nerd, dressed, like, tom, cruise, actio...   \n",
      "1  [art, podcast, named, taste, buds, hosted, two...   \n",
      "2  [8k, photo, shark, beach, teeth, eyes, colors,...   \n",
      "3  [angle, body, group, cyberpunk, 2077, npcs, 6t...   \n",
      "4                  [artwork, michel, granger, skull]   \n",
      "\n",
      "                                  Adjectives Removed  \\\n",
      "0  [tech, nerd, dressed, like, tom, cruise, actio...   \n",
      "1  [art, podcast, named, taste, buds, hosted, two...   \n",
      "2  [8k, photo, shark, beach, eyes, colors, bystan...   \n",
      "3  [angle, body, group, cyberpunk, 2077, npcs, 6t...   \n",
      "4                  [artwork, michel, granger, skull]   \n",
      "\n",
      "                                      Stemmed Tokens  \n",
      "0  [tech, nerd, dress, like, tom, cruis, action, ...  \n",
      "1  [art, podcast, name, tast, bud, host, two, guy...  \n",
      "2  [8k, photo, shark, beach, teeth, eye, color, b...  \n",
      "3  [angl, bodi, group, cyberpunk, 2077, npc, 6th,...  \n",
      "4                  [artwork, michel, granger, skull]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('PromptsToAnalyze.csv')\n",
    "\n",
    "# Create an instance of the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Function to remove adjectives and stopwords\n",
    "def remove_adjectives_stopwords(tokenized_text):\n",
    "    pos_tagged_text = pos_tag(tokenized_text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word.lower() for word, tag in pos_tagged_text if tag not in ['JJ', 'JJR', 'JJS'] and word.lower() not in stop_words]\n",
    "\n",
    "# Function to stem the words\n",
    "def stem_tokens(tokenized_text):\n",
    "    return [stemmer.stem(word) for word in tokenized_text]\n",
    "\n",
    "# Apply tokenization and adjective removal to the 'Prompt' column\n",
    "df['Tokenized Text'] = df['Prompt'].apply(tokenize_text)\n",
    "df['Tokenized Text'] = df['Tokenized Text'].apply(remove_adjectives_stopwords)\n",
    "\n",
    "# Create a new column 'Adjectives Removed' by removing adjectives and stopwords from 'Tokenized Text'\n",
    "df['Adjectives Removed'] = df['Tokenized Text'].apply(remove_adjectives_stopwords)\n",
    "\n",
    "# Create a new column 'Stemmed Tokens' by stemming the words in 'Tokenized Text'\n",
    "df['Stemmed Tokens'] = df['Tokenized Text'].apply(stem_tokens)\n",
    "\n",
    "# Print the first few rows of the DataFrame to check that it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tokens.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis for each prompt\n",
    "sentiment_scores = []\n",
    "for prompt in prompts:\n",
    "    scores = sia.polarity_scores(prompt)\n",
    "    sentiment_scores.append(scores)\n",
    "\n",
    "# Create a new DataFrame with the sentiment scores and prompts\n",
    "sentimentAnalysis_df = pd.DataFrame(sentiment_scores)\n",
    "sentimentAnalysis_df.columns = ['Negative', 'Neutral', 'Positive', 'Compound']\n",
    "sentimentAnalysis_df['Prompt'] = prompts  # Add the 'Prompt' column\n",
    "sentimentAnalysis_df = sentimentAnalysis_df[['Prompt', 'Negative', 'Neutral', 'Positive', 'Compound']]  # Reorder columns\n",
    "\n",
    "print(sentimentAnalysis_df.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting a pie chart of the sentiment distribution\n",
    "sizes = sentimentAnalysis_df[['Positive', 'Negative', 'Neutral']].mean()\n",
    "plt.figure(figsize=(6, 6))\n",
    "labels = ['Positive', 'Negative', 'Neutral']\n",
    "colors = ['#66b3ff', '#ff9999', '#99ff99']\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a DataFrame named 'sentimentAnalysis_df' with a column named 'Compound'\n",
    "# Extract the compound scores\n",
    "compound_scores = sentimentAnalysis_df['Compound']\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(compound_scores, bins=10, edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Compound Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Compound Scores')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the 'output_data' DataFrame\n",
    "\n",
    "# Define the output file path\n",
    "sentiment_output_file = 'sentiment_analysis.csv'\n",
    "\n",
    "# Write the output DataFrame to a new CSV file\n",
    "sentimentAnalysis_df.to_csv(sentiment_output_file, index=False)\n",
    "print(f\"Sentiment analysis saved to {sentiment_output_file}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize the prompts, remove stopwords, and count word frequencies\n",
    "tokenized_words = []\n",
    "for rows in df['Tokenized Text']:\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in tokens if word not in stop_words]\n",
    "    tokenized_words.extend(filtered_words)\n",
    "\n",
    "word_freq = Counter(tokenized_words)\n",
    "\n",
    "# Sort word frequencies in descending order\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create a DataFrame for word frequencies\n",
    "wordFrequency_df = pd.DataFrame(sorted_word_freq, columns=['Word', 'Frequency'])\n",
    "\n",
    "wordFrequency_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Without Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Tokenize the prompts, remove stopwords, and count word frequencies\n",
    "tokenized_words = []\n",
    "for prompt in prompts:\n",
    "    tokenized_text = [word.lower() for word in word_tokenize(prompt)]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pos_tagged_text = pos_tag(tokenized_text)\n",
    "    filtered_words = [word for word, tag in pos_tagged_text if tag not in ['JJ', 'JJR', 'JJS'] and word not in stop_words]\n",
    "    tokenized_words.extend(filtered_words)\n",
    "\n",
    "word_freq = Counter(tokenized_words)\n",
    "\n",
    "# Sort word frequencies in descending order\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create a DataFrame for word frequencies\n",
    "wordFrequency_noAdjectives_df = pd.DataFrame(sorted_word_freq, columns=['Word', 'Frequency'])\n",
    "\n",
    "wordFrequency_noAdjectives_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_words = wordFrequency_df.head(10)\n",
    "\n",
    "# Create a bar plot of the word frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_words['Word'], top_10_words['Frequency'])\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Most Frequent Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_nonAdjectives = wordFrequency_noAdjectives_df.head(10)\n",
    "\n",
    "# Create a bar plot of the word frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_words['Word'], top_10_words['Frequency'])\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Most Frequent Words (No Adjectives)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_25_words = wordFrequency_df.head(25)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(top_25_words['Frequency'], top_25_words['Word'], s=100, c='b', alpha=0.7)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Word Frequencies')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Output to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "output_file = 'word_frequencies1.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Word frequencies saved to {output_file}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def replace_words(text, replacements):\n",
    "    pattern = re.compile(r'\\b(?:%s)\\b' % '|'.join(replacements), re.IGNORECASE)\n",
    "    return re.sub(pattern, lambda match: replacements[match.group(0).lower()], text)\n",
    "\n",
    "# Define the replacements\n",
    "replacements = {\n",
    "    'girl': 'woman',\n",
    "    'female': 'woman',\n",
    "    'boy': 'man',\n",
    "    'male': 'man'\n",
    "}\n",
    "\n",
    "# Apply the replacements and tokenize the modified text\n",
    "adjectiveFree_df = df.copy()\n",
    "adjectiveFree_df['Prompt'] = adjectiveFree_df['Prompt'].apply(replace_words, replacements=replacements)\n",
    "\n",
    "# Tokenize the text\n",
    "adjectiveFree_df['Tokenized_Text'] = adjectiveFree_df['Prompt'].apply(word_tokenize)\n",
    "\n",
    "'''toRemoveWords = ['red', 'orange', 'yellow', 'green', \n",
    "                 'blue', 'purple', 'black', 'white', \n",
    "                 'grey', 'gray',\n",
    "                 'style', 'background', 'dark', 'lighting']\n",
    "'''\n",
    "              \n",
    "# Remove stopwords, punctuation, and color names\n",
    "# stop_words = set(stopwords.words('english') + list(string.punctuation) + toRemoveWords)\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Removing Adjectives\n",
    "def remove_adjectives(pos_tagged_text):\n",
    "    filtered_text = []\n",
    "    for word, tag in pos_tagged_text:\n",
    "        if tag not in ['JJ', 'JJR', 'JJS'] and word.lower() not in stop_words:\n",
    "            filtered_text.append(word.lower())\n",
    "    return filtered_text\n",
    "\n",
    "adjectiveFree_df['POS_Tagged_Text'] = adjectiveFree_df['Tokenized_Text'].apply(pos_tag)\n",
    "adjectiveFree_df['Filtered_Text'] = adjectiveFree_df['POS_Tagged_Text'].apply(remove_adjectives)\n",
    "\n",
    "print(adjectiveFree_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
